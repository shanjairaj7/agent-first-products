{
  "name": "Groq",
  "slug": "groq",
  "description": "Ultra-fast LLM inference on custom LPU hardware with an OpenAI-compatible API. Provides the fastest inference available for popular models. Python and JavaScript SDKs with a free tier.",
  "website": "https://groq.com",
  "logoUrl": "https://avatars.githubusercontent.com/u/79576057?s=64",
  "category": "compute",
  "agentFirstScore": 9,
  "interfaces": {
    "api": true,
    "sdk": true,
    "cli": false,
    "mcp": false,
    "webhook": false,
    "graphql": false
  },
  "signup": {
    "method": "website-bot-friendly",
    "hasAgentAuth": true,
    "allowsBots": true
  },
  "allFeaturesViaAPI": true,
  "sdkLanguages": ["python", "javascript"],
  "mcpServerUrl": null,
  "pricing": {
    "hasFree": true,
    "model": "usage-based"
  },
  "tags": ["llm-inference", "fast", "lpu", "openai-compatible"],
  "addedAt": "2026-02-28T00:00:00.000Z",
  "verified": false
}
